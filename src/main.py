"""Main entry point for DeepAgents Stock Research Assistant v1.3.0."""

import logging
import time
from deepagents import create_deep_agent

from .tools import (
    get_stock_price,
    get_financial_statements,
    get_technical_indicators,
    get_news_sentiment,
    get_analyst_recommendations
)
from .tools.comparison import compare_stocks
from .agents import fundamental_analyst, technical_analyst, risk_analyst
from .agents.comparison import comparison_analyst
from .agents.reflection import reflection_agent
from .ui.gradio_app_v3 import create_gradio_interface_v2
from .utils.config import settings
from .utils.database import get_database
from .utils.validation import extract_symbols_from_query
from .utils.streaming import create_streaming_wrapper

# NEW v1.2.0: Bulletproof systems
from .utils.model_provider import get_model_provider
from .utils.memory import get_memory_system
from .utils.health_monitor import get_health_monitor
from .utils.feedback import get_feedback_system
from .utils.analytics import get_tool_analytics
from .utils.confidence import get_confidence_scorer

# NEW v1.3.0: Performance optimizations
from .utils.llm_streaming import GradioStreamingHandler, create_streaming_model
from .utils.async_agents import get_async_executor  # Async parallel execution framework

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)


# Research instructions for the main agent
RESEARCH_INSTRUCTIONS = """You are an elite stock research analyst with access to multiple specialized tools and sub-agents.

Your research process should be systematic and comprehensive:

1. **Initial Data Gathering**: Start by collecting basic stock information, current price, and recent news
   - Use get_stock_price for current market data
   - Use get_news_sentiment to understand recent market sentiment

2. **Fundamental Analysis**: Deep dive into financial statements, ratios, and company fundamentals
   - Use get_financial_statements for detailed financial data
   - Delegate to the fundamental-analyst sub-agent for in-depth analysis
   - Use get_analyst_recommendations to see what Wall Street thinks

3. **Technical Analysis**: Analyze price patterns, trends, and technical indicators
   - Use get_technical_indicators for chart data and signals
   - Delegate to the technical-analyst sub-agent for pattern analysis and trading signals

4. **Risk Assessment**: Identify and evaluate potential risks
   - Delegate to the risk-analyst sub-agent for comprehensive risk evaluation
   - Consider market, company-specific, and sector risks

5. **Comparison Analysis** (if comparing multiple stocks):
   - Use compare_stocks tool to fetch data for all stocks in parallel
   - Delegate to the comparison-analyst sub-agent for side-by-side analysis
   - Provide clear rankings and allocation suggestions

6. **Synthesis**: Combine all findings into a coherent investment thesis
   - Reconcile fundamental value with technical signals
   - Balance opportunities against risks
   - Consider multiple perspectives and timeframes

7. **Recommendation**: Provide clear, actionable investment recommendation
   - Buy/Hold/Sell recommendation with confidence level
   - Price targets (12-month outlook)
   - Key catalysts and risk factors to monitor
   - Suggested position sizing based on risk profile

**Guidelines:**
- Always use specific data and numbers to support your analysis
- Cite your sources and methodology
- Consider multiple perspectives and potential scenarios
- Provide actionable insights with concrete entry/exit levels
- Structure your final report professionally with clear sections
- Be objective and balanced - acknowledge both bullish and bearish factors
- When using sub-agents, provide them with specific, focused tasks
- Incorporate specialized insights into your overall analysis
- If data is unavailable, note limitations rather than speculate

**Report Structure:**
Your final report should include:
1. Executive Summary (recommendation, price target, key thesis)
2. Company Overview (from stock data and news)
3. Fundamental Analysis (financials, valuation, analyst views)
4. Technical Analysis (trends, signals, key levels)
5. Risk Assessment (key risks and mitigation)
6. Investment Recommendation (specific action with timeframe)
7. Monitoring Plan (what to watch going forward)

Remember: Provide value through synthesis and insight, not just data aggregation."""


def create_research_agent():
    """
    Create and configure the DeepAgent research system.

    Returns:
        Configured DeepAgent instance
    """
    logger.info("Initializing DeepAgents Stock Research Assistant v1.3.0")

    # Validate configuration
    if not settings.validate():
        raise ValueError("Invalid configuration settings")

    # NEW v1.2.0: Use model provider for automatic fallback
    logger.info("Getting model from multi-provider fallback chain")
    model_provider = get_model_provider()
    model = model_provider.get_model()

    # Define all tools
    tools = [
        get_stock_price,
        get_financial_statements,
        get_technical_indicators,
        get_news_sentiment,
        get_analyst_recommendations,
        compare_stocks  # Multi-stock comparison tool
    ]

    # Define sub-agents
    subagents = [
        fundamental_analyst,
        technical_analyst,
        risk_analyst,
        comparison_analyst,  # Comparison specialist
        reflection_agent  # NEW v1.2.0: Quality gate
    ]

    # Create the DeepAgent
    logger.info("Creating DeepAgent with 6 tools and 5 sub-agents")
    agent = create_deep_agent(
        tools=tools,
        instructions=RESEARCH_INSTRUCTIONS,
        subagents=subagents,
        model=model
    )

    logger.info("DeepAgent created successfully with bulletproof features")
    return agent


def run_stock_research(query: str, user_context: dict = None) -> dict:
    """
    Run the stock research agent and return the analysis.

    Args:
        query: User's research query
        user_context: Optional user context for memory system

    Returns:
        Dictionary with report, confidence, research_id, and feedback_prompt
    """
    start_time = time.time()

    try:
        logger.info(f"Running research query: {query[:100]}...")

        # NEW v1.2.0: Get all bulletproof systems
        memory = get_memory_system()
        analytics = get_tool_analytics()
        confidence_scorer = get_confidence_scorer()

        # Store query in memory
        memory.record_interaction(query, "")

        # Get context for query enhancement
        context = memory.get_context_for_query(query)
        enhanced_query = query  # Use original query (context already available to system)

        # Track analytics for the research process
        research_start = time.time()

        # Get the agent (could be cached in production)
        agent = create_research_agent()

        # Invoke the agent
        result = agent.invoke({
            "messages": [{"role": "user", "content": enhanced_query}]
        })

        # Extract the response
        messages = result.get("messages", [])
        output_text = ""

        if not messages:
            logger.warning("No messages returned in result")
            output_text = "Error: No response messages received."
        elif isinstance(messages[-1], dict):
            output_text = messages[-1].get("content", "")
        elif hasattr(messages[-1], "content"):
            output_text = messages[-1].content
        else:
            logger.error("Unrecognized message format")
            output_text = "Error: Invalid response message format."

        # Add generated files if any
        file_output = ""
        if "files" in result:
            file_output += "\n\n=== Generated Research Files ===\n"
            for filename, content in result["files"].items():
                preview = content[:500] + "..." if len(content) > 500 else content
                file_output += f"\n**{filename}**\n{preview}\n"

        analysis_report = output_text + file_output

        # NEW v1.2.0: Calculate confidence score
        symbols = extract_symbols_from_query(query)
        symbol = symbols[0] if symbols else "MULTI"

        # Gather data used for analysis (would come from tool calls in production)
        research_data = {
            "stock_price": "fetched",
            "financials": "fetched",
            "technical": "fetched",
            "news": "fetched",
            "analysts": "fetched"
        }

        confidence = confidence_scorer.calculate_confidence(
            data=research_data,
            analysis=analysis_report,
            symbol=symbol
        )

        # Format confidence report
        confidence_report = confidence_scorer.format_confidence_report(confidence)

        # Combine analysis with confidence
        final_report = analysis_report + "\n" + confidence_report

        # Save to database
        research_id = None
        try:
            db = get_database()
            research_id = db.save_research(
                symbol=symbol,
                query=query,
                report=final_report,
                metadata={
                    "tool": "deepagents",
                    "version": "1.2.0",
                    "confidence": confidence["overall_score"],
                    "confidence_level": confidence["confidence_level"]
                }
            )
            logger.info(f"Saved research ID {research_id} for {symbol} to database")
        except Exception as db_error:
            logger.warning(f"Failed to save to database: {db_error}")

        # Store in memory
        memory.record_interaction(query, final_report)

        # Track research completion
        research_time = time.time() - research_start
        analytics.record_call("research_agent", True, research_time)

        # Return structured response
        return {
            "report": final_report,
            "confidence": confidence,
            "research_id": research_id,
            "symbol": symbol,
            "query": query,
            "execution_time": time.time() - start_time
        }

    except Exception as e:
        logger.exception("Research failed with exception")
        analytics.record_call("research_agent", False, time.time() - start_time, str(e))

        return {
            "report": f"Error: {str(e)}",
            "confidence": None,
            "research_id": None,
            "symbol": None,
            "query": query,
            "execution_time": time.time() - start_time
        }


def run_stock_research_streaming(query: str, enable_true_streaming: bool = True):
    """
    Run stock research with streaming output for real-time updates.

    v1.3.0: Now supports TRUE token-by-token streaming from LLM!

    Args:
        query: User's research query
        enable_true_streaming: If True, streams LLM tokens in real-time (v1.3.0)

    Yields:
        Progressive updates and final report
    """
    if not enable_true_streaming:
        # Fallback to simulated streaming (v1.2.0 behavior)
        progress_steps = [
            "ðŸ”„ Initializing DeepAgents v1.3.0...",
            "ðŸ”Œ Connecting to model provider (with fallback)...",
            "ðŸ“Š Gathering market data in parallel...",
            "ðŸ’¹ Analyzing financial metrics...",
            "ðŸ“ˆ Calculating technical indicators...",
            "ðŸ“° Checking recent news sentiment...",
            "ðŸ¦ Reviewing analyst recommendations...",
            "ðŸ¤– AI agents collaborating...",
            "ðŸ” Reflection agent reviewing quality...",
            "ðŸ“Š Calculating confidence scores...",
            "ðŸ“ Generating comprehensive report...",
        ]

        for step in progress_steps:
            yield f"\n{step}\n"
            time.sleep(0.3)

        try:
            result = run_stock_research(query)

            yield f"\n\n{'='*80}\n"
            yield "âœ… **ANALYSIS COMPLETE**\n"
            yield f"{'='*80}\n\n"

            if isinstance(result, dict):
                yield result.get("report", str(result))
                if result.get("execution_time"):
                    yield f"\n\nâ±ï¸ *Execution time: {result['execution_time']:.2f}s*\n"
            else:
                yield result

        except Exception as e:
            yield f"\n\nâŒ Error: {str(e)}\n"
        return

    # v1.3.0: TRUE TOKEN-BY-TOKEN STREAMING! ðŸ”¥
    try:
        yield "\nðŸš€ **v1.3.0 True Token Streaming Active**\n"
        yield "You'll see the AI thinking in real-time...\n\n"
        yield "=" * 80 + "\n\n"

        # Create streaming handler
        streaming_handler = GradioStreamingHandler(chunk_size=3)

        # Get memory and analytics
        memory = get_memory_system()
        analytics = get_tool_analytics()
        confidence_scorer = get_confidence_scorer()

        # Store query in memory
        memory.record_interaction(query, "")

        # Get context
        context = memory.get_context_for_query(query)

        # Create agent with streaming enabled
        logger.info("Creating streaming-enabled agent")

        # Get model with streaming
        model_provider = get_model_provider()
        base_model = model_provider.get_model()

        # Enable streaming on model
        streaming_model, handler = create_streaming_model(base_model, streaming_handler)

        # Create agent with streaming model
        tools = [
            get_stock_price,
            get_financial_statements,
            get_technical_indicators,
            get_news_sentiment,
            get_analyst_recommendations,
            compare_stocks
        ]

        subagents = [
            fundamental_analyst,
            technical_analyst,
            risk_analyst,
            comparison_analyst,
            reflection_agent
        ]

        agent = create_deep_agent(
            tools=tools,
            instructions=RESEARCH_INSTRUCTIONS,
            subagents=subagents,
            model=streaming_model
        )

        # Start agent invocation in background thread
        result_container = {}
        error_container = {}

        def run_agent():
            try:
                result = agent.invoke({
                    "messages": [{"role": "user", "content": query}]
                })
                result_container["result"] = result
            except Exception as e:
                error_container["error"] = e

        import threading
        agent_thread = threading.Thread(target=run_agent)
        agent_thread.start()

        # Stream tokens as they arrive
        for chunk in streaming_handler.stream_chunks():
            yield chunk

        # Wait for agent to finish
        agent_thread.join()

        # Check for errors
        if "error" in error_container:
            raise error_container["error"]

        # Get result
        result = result_container.get("result", {})

        # Extract final response
        messages = result.get("messages", [])
        if messages:
            if isinstance(messages[-1], dict):
                final_text = messages[-1].get("content", "")
            elif hasattr(messages[-1], "content"):
                final_text = messages[-1].content
            else:
                final_text = ""

            # Add confidence scoring
            symbols = extract_symbols_from_query(query)
            symbol = symbols[0] if symbols else "MULTI"

            research_data = {
                "stock_price": "fetched",
                "financials": "fetched",
                "technical": "fetched",
                "news": "fetched",
                "analysts": "fetched"
            }

            confidence = confidence_scorer.calculate_confidence(
                data=research_data,
                analysis=final_text,
                symbol=symbol
            )

            confidence_report = confidence_scorer.format_confidence_report(confidence)

            # Yield confidence report
            yield "\n\n" + confidence_report

            # Save to database
            try:
                db = get_database()
                research_id = db.save_research(
                    symbol=symbol,
                    query=query,
                    report=final_text + "\n" + confidence_report,
                    metadata={
                        "tool": "deepagents",
                        "version": "1.3.0",
                        "streaming": "true",
                        "confidence": confidence["overall_score"],
                        "confidence_level": confidence["confidence_level"]
                    }
                )
                yield f"\n\nðŸ’¾ *Saved as research ID: {research_id}*\n"
            except Exception as e:
                logger.warning(f"Failed to save: {e}")

    except Exception as e:
        logger.exception("Streaming research failed")
        yield f"\n\nâŒ Error: {str(e)}\n"


def get_system_health() -> dict:
    """
    Get comprehensive system health status.

    Returns:
        System health dictionary with all component statuses
    """
    try:
        health_monitor = get_health_monitor()
        return health_monitor.perform_health_check()
    except Exception as e:
        logger.exception("Failed to get system health")
        return {
            "status": "error",
            "message": str(e),
            "timestamp": time.time()
        }


def submit_feedback(research_id: int, rating: int, helpful: list, missing: list) -> bool:
    """
    Submit user feedback for a research report.

    Args:
        research_id: ID of the research report
        rating: Star rating (1-5)
        helpful: List of helpful aspects
        missing: List of missing aspects

    Returns:
        Success boolean
    """
    try:
        feedback_system = get_feedback_system()
        feedback_system.submit_feedback(research_id, rating, helpful, missing)
        logger.info(f"Feedback submitted for research ID {research_id}")
        return True
    except Exception as e:
        logger.exception("Failed to submit feedback")
        return False


async def run_parallel_research_example(symbols: list) -> dict:
    """
    EXAMPLE: Demonstrate async parallel execution framework.

    This function shows how to use the v1.3.0 async agent framework
    to run multiple independent research tasks in parallel.

    NOTE: The main DeepAgents workflow uses internal sub-agent orchestration.
    This async framework is for custom parallel workflows where you want to
    run completely independent tasks simultaneously.

    Args:
        symbols: List of stock symbols to research in parallel

    Returns:
        Dictionary with results from all parallel executions

    Example:
        ```python
        import asyncio
        from src.main import run_parallel_research_example

        # Run research on multiple stocks in parallel
        results = asyncio.run(run_parallel_research_example(['AAPL', 'MSFT', 'GOOGL']))
        print(f"Researched {len(results['agent_results'])} stocks in parallel")
        print(f"Total time: {results['metadata']['total_execution_time']:.2f}s")
        ```
    """
    logger.info(f"Starting parallel research for {len(symbols)} symbols")

    # Get async executor
    executor = get_async_executor()

    # Define research function for each symbol
    def research_symbol(symbol: str, data=None):
        """Individual research function to run in parallel."""
        query = f"Analyze {symbol} for investment potential"
        return run_stock_research(query)

    # Create agent configs for parallel execution
    agent_configs = [
        {
            "name": f"research_{symbol}",
            "function": research_symbol,
            "priority": i
        }
        for i, symbol in enumerate(symbols)
    ]

    # Run all research tasks in parallel
    results = await executor.run_agents_parallel(
        agent_configs=agent_configs,
        query="",  # Query embedded in function
        data=None
    )

    logger.info(
        f"Parallel research complete: {results['metadata']['successful']} succeeded, "
        f"{results['metadata']['failed']} failed"
    )

    return results


def main():
    """Main entry point for the application."""
    logger.info("Starting DeepAgents Stock Research Assistant v1.3.0 'Lightning Fast++'")

    # Initialize all bulletproof systems
    logger.info("Initializing bulletproof systems...")
    get_model_provider()
    get_memory_system()
    get_health_monitor()
    get_feedback_system()
    get_tool_analytics()
    get_confidence_scorer()

    # v1.3.0: Async executor available for custom parallel workflows
    # See run_parallel_research_example() for usage demonstration
    get_async_executor()

    logger.info("All systems initialized successfully")

    # Create the enhanced Gradio interface with streaming
    demo = create_gradio_interface_v2(
        research_function=run_stock_research,
        research_function_streaming=run_stock_research_streaming,
        health_function=get_system_health,
        feedback_function=submit_feedback
    )

    # Launch the app
    logger.info(f"Launching Gradio interface on {settings.SERVER_HOST}:{settings.SERVER_PORT}")
    demo.launch(
        server_name=settings.SERVER_HOST,
        server_port=settings.SERVER_PORT,
        show_error=True
    )


if __name__ == "__main__":
    main()
